\chapter{Error measures and metrics}
Mesh adaptivity requires an error measure to guide it. Hierarchical
mesh refinement algorithms are typically guided by error measures
which associate a scalar to each element, measuring the contribution
of that element to the error in some quantity of interest. Anisotropic
mesh adaptivity \citep{simpson1994, buscaglia1997, pain2001, li2005,
  frey2005} requires directional information at each point in the
domain, and thus is typically guided by an error metric, a tensor
field defined over the domain.  Anisotropic mesh adaptivity exploits
any anisotropic features of the solution fields to be represented,
increasing the computational efficiency of the simulation by
minimising the number of nodes and elements required to adequately
represent the solution fields.

In one dimension, the interpolation error over a linear element is
bounded by the second derivative of the function being
interpolated. This result carries over to higher dimensions, with the
second derivative replaced by the Hessian of the function being
interpolated, the tensor field of second partial derivatives
\citep{frey2005}. The Hessian describes the curvature of the solution
at each point in every direction; using the Hessian for the error
metric to guide anisotropic mesh adaptivity places resolution where
the solution field curves. For elliptic problems, C\'ea's lemma shows
us that the interpolation error bounds the discretisation error in the
energy norm; in practice, minimising the interpolation error in this
manner has been found to be highly effective at controlling the
discretisation error, even for non-elliptic problems.

In the previous sections mesh optimisation operations and an objective
functional have been introduced.  These can be used together to
formulate an optimisation approach to deliver optimally shaped
elements within a given domain. The element quality depends upon the
area and edge lengths of the element and these scalars were defined in
terms of a metric $M$. Assuming that this is the standard Euclidean
metric then this procedure would seek to deliver uniformly sized
equilateral triangles in the domain. To turn this procedure into one
which delivers variable anisotropic resolution a non-Euclidean metric
$M({\bf x})$ can be used. Given element $\triangle$ one may assume,
both for simplicity of explanation and also numerical implementation,
that $M$ is constant over $\triangle$. In this case
\begin{equation*}
|\triangle|_M = \sqrt{\textrm{det}(M)}|\triangle|_E,\quad
|\partial \triangle|_M = \sum_{i=1}^{3} ||\partial \pmb{e}_i||_M = \sum_{i=1}^{3} \sqrt{\pmb{e}_i^T M {\bf e}_i},
\end{equation*}
where $|\triangle|_E$ is the area of element $\triangle$ in the
standard Euclidean metric, $\pmb{e}_i$ is the vector representing the
$i$-th edge of element $\triangle$ and $||\partial \pmb{e}_i||_M$ is
its length with respect to the metric $M$.

The choice of $M$ is critical in achieving an adapted mesh that in
some heuristic sense is optimal for given solution characteristics.
Interpolation error estimates form an excellent starting point for the
construction of a suitable metric $M$.  In one dimension, the linear
interpolation error of a smooth function is bounded by the product of
the second derivative of the function being interpolated and the size
of the region being considered. For example, for a finite element
approximation this would be the size of the element the
piecewise-linear interpolant is defined over.  In higher dimensions a
similar result holds with the second derivative replaced by the
Hessian of the function being interpolated, the tensor field of second
partial derivatives \citep{dazevedo1991,george1998,frey2005}. The
Hessian describes the curvature of the solution at each point in every
direction; using the Hessian as a basis for the metric $M$ results in
enhanced resolution where the solution field has high curvature.

For a mesh of triangles in 2D, assume a sufficiently smooth function
$u\equiv u(\pmb{x})$ is approximated by its piecewise-linear Lagrange
interpolant $\Pi_h u$ on element $\triangle$. The interpolation error
on element $\triangle$ satisfies
\begin{equation}
\epsilon_u\equiv ||{u} - \Pi_h{u}||_{\infty,\triangle}
\le c\cdot
\max_{\pmb{x}\in \triangle}\max_{\pmb{v}\subset \triangle} \pmb{v}^T |H| \pmb{v}
\le c \cdot\max_{\pmb{e}\in E_{\triangle}}\pmb{e}^T |\bar{H}| \pmb{e},
\label{highDint}
\end{equation}
where $c$ is a constant independent of the mesh, $\pmb{v}$ is a vector
in $\triangle$, $E_{\triangle}$ is the set of edges of $\triangle$,
$||\cdot||_{\infty,\triangle}$ is the infinity norm over $\triangle$,
and $\bar{H}$ is an element-valued Hessian defined such that
(\ref{highDint}) holds \citep{frey2005}. $|H|$ denotes the positive
definite metric formed by taking the absolute value of the eigenvalues
of ${H}$ and reflects the fact that it is the magnitude of the
curvature that is of interest, rather than the sign of the curvature.

Alternatively, the square of the eigenvalues can be used; this
optimises for the gradient error \citep{dazevedo1991, shewchuk2002}
and leads to elements with larger aspect ratios.

For a target, possibly spatially and temporally varying, interpolation
error, $\epsilon_u$, a metric $M$ is defined such that, assuming
$c=1$, an element edge length is unity with respect to $M$ if it has
the desired interpolation error, \ie\
\begin{equation}
  M = \frac{1}{\epsilon_u} {|\bar{H}|}.
  \label{eqn:ME}
\end{equation}
Construction of an adapted mesh which equidistributes the
interpolation error throughout the domain can be seen from
(\ref{highDint}) to be equivalent to the construction of a uniform
mesh of equilateral tetrahedra, \ie\ all element edge lengths are
unity with respect to $M$. It is the element quality that defines how
close to this requirement of equidistribution each element is.

The third and final $p$--metric, $M_p$ is given by \citep{chen2007optimal}

\begin{equation}\label{eq:metric_l2}
M_p({\bf x}) = \frac{1}{\epsilon({\bf x})}(\det(|H({\bf
x})|))^{-\frac{1}{2p+n}}|H({\bf x})| = (\det(|H({\bf
x})|))^{-\frac{1}{2p+n}}M_\infty \, ,
\end{equation}

where $n$ is the dimension of the space and $p \in \mathbb{Z}^+$. With
this metric, the $L_p$ norm of the interpolation error is bounded by

\begin{equation}
||f({\bf x})-\delta({\bf x})||_p = \left( \int_\Omega |f({\bf
x})-\delta({\bf x})|^{p} \right) ^{1/p} \leq CN^{-2/n}||^n \sqrt{\det
\, |H({\bf x})|}||_{\frac{pn}{2p+n}(\Omega)} \, ,
\end{equation}

where $N$ is the number of elements in the mesh
\citep{adaptivity_loseille_10-ii}. Note that in the limit \mbox{$p \to
  \infty\, , \, (\det(|H|))^{-\frac{1}{2p+n}} \to 1$} and the absolute
metric, $M_\infty$, is recovered.

A symmetric positive-definite metric tensor $M$ may be decomposed as
\begin{equation*}
M = Q \Lambda Q^T,
\end{equation*}
where $\Lambda$ is the diagonal matrix consisting of the eigenvalues
of $M$ (the principal curvatures of the solution field for $M=H$) and
$Q$ is an ortho-normal matrix of eigenvectors ${Q}^i$. $Q$ may be
interpreted geometrically as a rotation and $\Lambda$ a rescaling.
The eigenvalue $\lambda^i$ encodes the desired edge length in the
direction ${Q}^i$ as
\begin{equation*}
h^i = 1 / \sqrt{{\lambda}^i},
\end{equation*}
\ie\ anisotropic as well as inhomogeneous resolution results from a
mesh that respects the metric $M$. Note that the transformation
${\bmv}=Q\sqrt{|\Lambda|}^{-1}\tilde{\bmv}$ may be used to transform a
uniform isotropic element in a computational space into an adapted
anisotropic element in the physical domain achieving the desired
interpolation error everywhere,
Fig. \ref{Fig:Transformed_Elements}. This can be seen from the
decomposition of $M$ above and
\begin{equation*}
1={\bmv}^T  M {\bmv} = (\sqrt{|\Lambda|}Q^T\bmv)^T(\sqrt{|\Lambda|}Q^T\bmv)=\tilde{\bmv}^T\tilde{\bmv}.
\end{equation*}

\begin{figure}
\centering
\includegraphics[width=12cm]{images/Transformed_Elements.pdf}
\caption{Mapping of triangular elements between physical and metric
  space.  Left: those elements in physical space which are of the
  appropriate size and aspect ratio, and which are of maximal area,
  can be shown to map to the unit equilateral triangle in metric space
  under the transformation $S=Q\sqrt{|\Lambda|}^{-1}$.  Right: an
  element which does not maximise area and consequently transforms to
  a non-equilateral triangle in metric space.}
\label{Fig:Transformed_Elements}
\end{figure}

\citep{george1998} (pg. 350) point out that use of a global error norm
can in fact alter the error analysis by enhancing the accuracy of
regions with high solution magnitude where changes are clearer than
those regions of low magnitude. To overcome this, it is possible to
use a {\it relative error},
\begin{equation*}\label{eqn:relative_error_3D}
  \epsilon_r=\frac{\pmb{v}^T \pmb{H} \pmb{v}}{\max(\|\psi\|, \psi_{min})},
\end{equation*}
giving a new metric tensor,
\begin{equation*}\label{eqn:relative_metric}
  \pmb{M}_r = \frac{\vert\pmb{H}\vert}{\epsilon_r\max(\|\psi\|, \psi_{min})},
\end{equation*}
for some positive $\psi_{min}$. $\psi_{min}$ is required to be some
number greater than zero to avoid division by zero, but it can also be
used to suppress curvature values associated with small values of
$\psi$ which may be considered spurious. It is important to note that
while in (\ref{eqn:ME}), the interpolation error, $\epsilon_g$, has
the same dimensions as the solution field, the relative interpolation
error, $\epsilon_r$, is dimensionless and $100\epsilon_r$ is the
percentage error.


\section{Combining multiple fields and bounding aspect ratio and edge lengths}

For complex problems one will wish to simultaneously adapt the mesh
for several solution fields. The approach taken here is to use a
single mesh to represent all fields and hence typically a compromise
needs to made. Often this compromise will not be substantial since in
many problems regions and directions of high curvature are common
between solution fields. For oceanographic problems the base set of
fields we would want to adapt to are the components of velocity,
temperature and salinity. Metric fields are constructed for each with
their own interpolation errors. A superposition procedure is then
undertaken so that a single metric results which respects the edge
length and requirements of each individual metric. The procedure is
shown graphically in figure \ref{Fig:MergedMetrics} and follows
\citep{pain2001}, see also \citep{castrodiaz1997,george1998}.

\begin{figure}
\centering
\includegraphics[width=5cm]{images/MergedMetrics.pdf}
\caption{The superposition of two metrics to yield a third which
  respects the edge-length requirements of both.  The metrics are
  shown here graphically as ellipses. The two original metrics are
  shown in faint lines, the new superimposed metric, or maximal inner
  ellipse, is represented by the bold line}
\label{Fig:MergedMetrics}
\end{figure}

In addition, it is sometimes advantageous to modify the metric to
impose maximum and minimum element sizes on the mesh: for example, to
stop the optimisation procedure from attempting to produce elements
larger than the solution domain, or smaller than a certain scale which
the user deems to be unimportant, or which they wish to use
subgrid-scale models to represent. Also, for problems which have known
high aspect ratio dynamics or domains it can be necessary to impose
these maximum and minimum constraints directionally. The approach
taken to achieve this here is to construct extra metrics describing
maximum and minimum edge lengths. For example, assuming the user
wished to limit the maximum element sizes in the x and y directions to
be $h_{x,\textrm{max}}$ and $h_{y,\textrm{max}}$ respectively, then
the metric $M_{\textrm{max}} =
\textrm{diag}((h_{x,\textrm{max}})^{-1/2},(h_{y,\textrm{max}})^{-1/2})$,
could be superimposed.  For the minimum edge length metric the $\max$
operator must be replaced with a $\min$ operator. More general
directions for the maximum and minimum edge lengths can be introduced
through the use of the rotation matrix $Q$.

To bound the aspect ratio of elements in physical space a limit on the
ratio of eigenvalues may also be performed. For example, to limit the
aspect ratio of elements to be below $a$ the eigenvalues of the metric
can be modified as follows:
\begin{equation*}
{\lambda}'_j=\max\left\{\tilde{\lambda}_j,\frac{1}{a^2}\max_{i=1,2}\tilde{\lambda}_i\right\},\;\; j=1,2,
\end{equation*}
\ie\ this operation seeks to limit the minimum eigenvalue, and hence
the maximum edge length in an element.  It thus achieves a maximum
bound on aspect ratio without compromising the interpolation error
achieved over that element.

\section{Hessian recovery}
We recall the definition of the Hessian.  Given a function
$u=u(\pmb{x}) \in C^2(\Omega)$ we define the Hessian $\pmb{H}$ as the
tensor of second-order partial derivatives, where its matrix
contributions are
\begin{equation} \label{eqn:hessian}
  \pmb{H}_{ij} = \frac{\partial^2u}{\partial x_i \partial x_j}.
\end{equation}
For $C^2$ functions, the Hessian is symmetric.

In practice only the discrete interpolant is available, not
an analytical formula for the function being interpolated. Thus it is
necessary to recover a discrete approximation to the true Hessian, and
use this to guide anisotropic adaptivity. Typically, we wish to obtain
a piecewise linear approximation to the Hessian.

Different recovery schemes such as superconvergent patch recovery
\citep{zienkiewicz1992}, integration by parts \citep{buscaglia1997}
and double lumped $L^2$ projection \citep{pain2001} have been
suggested. In this work we use quadratic fitting \citep{vallet2007}.

\section{Load balanced parallelisation}
The principal challenges associated with the parallelisation of a
computational model based on adaptive meshes are: consistency of
shared node and element information between subdomains; minimisation
of communication overhead costs; and dynamic load-balancing. Mesh data
consistency refers to the requirement that all processors that have
information regarding particular mesh entities (\ie\ nodes, elements,
auxiliary), have that information correctly updated as part of the
mesh adaptivity process, and that the complete mesh remains conformal
after the application of mesh optimisation (\ie\ no hanging nodes are
allowed). After mesh optimisation has been performed, load-imbalances
are to be expected as nodes and elements are in general added and
deleted according to solution requirements, thus the distribution of
work may vary across subdomains. The obvious exception to this is the
r-method as the number of nodes and mesh connectivity does not change
--- clearly an attractive property for parallel computation.  There is
a great deal of interest in dynamic load-balancing and cost modelling
for parallel mesh optimisation --- deciding when the mesh needs to be
repartitioned among the processors involved in the computation,
methods for calculating such a repartitioning and efficiently
realising the new partitioning through data migration (also referred
to as data remapping).

When mesh optimisation methods are applied in parallel, one must
consider what happens to mesh entities which are shared between
processes as optimisation uses Gauss-Seidel type sweeps of the entire
mesh, and the mesh must of course be conformal throughout the
processes. A method based on maximal independent sets is used to
accomplish this \citep{jones1997, freitag1999b, freitag1999}. By only
allowing mesh entities of the same colour to be updated within a
parallel mesh optimisation iteration, it can be ensured that the
shared mesh entities are not updated simultaneously on different
partitions. In this context, a node of the graph to be coloured is
defined by the set of nodes and elements which would be modified by a
local operation, while the edges of the graph is a superposition of
the mesh node and element adjacency lists --- this is the so called
task graph which of course changes for different elemental
operation. The task graph defines the mesh entities that must be
updated when an elemental operation is performed and thus defines the
data that must be synchronised at the end of an adaptive step. Broadly
speaking, the parallel mesh adaptivity method involves sweeping
through graph entities of the same colour in sequence, and then
synchronising changes after each colour has been processed
\citep{freitag1999}.
